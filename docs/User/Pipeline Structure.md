# Pipeline Structure

## Pipeline Builder
A pipeline is built using `IPipelineBuilder` which should be injected into the constructor of every pipeline container class.

A pipeline has an intentionally fixed structure, consisting of extract, transform, and load, in that order. Because `IPipelineBuilder` implements the fluent builder pattern, an IDE with Intellisense or similar can provide suggestions to make pipeline construction simpler.

```csharp
public class CustomerPipelines : IPipelineContainer
{
    private readonly IPipelineBuilder _pipelineBuilder;

    public CustomerPipeline(IPipelineBuilder pipelineBuilder)
    {
        _pipelineBuilder = pipelineBuilder;
    }

    public Pipeline Load()
    {
        return _pipelineBuilder
            .Extract()
            .Transform()
            .Load();
    }
}
```

## Entities
A data pipeline extracts, transforms, and loads data by entity data type. An entity is a class that describes the data structure at the source or target system.

Rowbot includes three built-in types that an entity can inherit from. These provide some common properties for loading data into a star schema.

- **Row** contains key and change hash fields that support change detection by loaders.
- **Dimension** inherits from **Row** and additionally provides date fields to record change history.
- **Fact** inherits from **Row** and additionally provides a created date field.

### Entity and Field Attributes

Every entity field is declared as a property of the entity class and each property can be decorated with attributes that further describe the field. Most supported field attributes are from the `System.ComponentModel.DataAnnotations` and `System.ComponentModel.DataAnnotations.Schema` namespaces. A few more specific attributes are included in the `Rowbot.Entities.DataAnnotations` namespace.

| Type | Attribute | Description |
|---|---|---|
| Class | Table | Declares the name of the dataset and optionally schema name (if the entity describes a database table) |
| Field | Column | Declares the column name and optionally order |
| Field | DatabaseGenerated | The value of the field is generated by the target system |
| Field | Key | The field is a primary key |
| Field | NotMapped | The field is not included when extracting or loading data |
| Field | Required | The field is required |
| Field | MaxLength | For a string or array type, changes the max size from default 300|
| Field | Precision | For a decimal type, specifies the precision and scale |

A field value is defined as nullable by making the property type nullable. There is no nullable field attribute.

### Example

```csharp
[Table("DimCustomers")]
public sealed class TargetCustomer : Dimension
{
    [Key]
    [DatabaseGenerated(DatabaseGeneratedOption.Identity)]
    public int CustomerKey { get; set; }
    public int Id { get; set; }
    [SlowlyChangingDimensionType2]
    [MaxLength(100)]
    public string? Name { get; set; }
    public bool Inactive { get; set; }
}
```

Note: `SlowlyChangingDimensionType2` is a custom attribute, specific to the built-in slowly changing dimension loader.

> Rowbot does not guarantee that custom connectors will support every common attribute. Attribute support is the responsibility of the custom connector developer.

## Suggested Structure
One suggested approach to pipeline design is to include all configuration within or very local to the pipeline definition. This transparency makes reading and understanding a pipeline faster and easier.

In the following example, we can see that data is extracted from the **SourceCustomer** table of **source.db**, it is transformed with property mappings that copy `CustomerId`, `CustomerName`, `CustomerAddress`, and `CustomerPhone` from the original data type `SourceCustomer` to properties `Id`, `Name`, `Address`, and `Phone` of the target data type `TargetCustomer`. Finally data is loaded to **target.db**, the target data type is inferred from the previous step in the pipeline - the user knows implicitly that `TargetCustomer` will be loaded to target.

```csharp
public class CustomerPipelines : IPipelineContainer
{
    private readonly IPipelineBuilder _pipelineBuilder;

    public CustomerPipeline(IPipelineBuilder pipelineBuilder)
    {
        _pipelineBuilder = pipelineBuilder;
    }

    public Pipeline Load()
    {
        return _pipelineBuilder
            .ExtractSqlite<SourceCustomer>(
                "Data Source=.\\source.db",
                "SELECT TOP (@BatchSize) [Id],[Name],[Address],[Phone] 
                FROM [Source].[dbo].[SourceCustomer] 
                WHERE [Id] > @Id 
                ORDER BY [Id]")
            .WithCursorPagination(x => x.Id)
            .Transform<TargetCustomer>(
                (source, mapper) =>
                {
                    return source
                        .Select(x => mapper.Apply(new TargetCustomer
                        {
                            CustomerId = x.Id,
                            CustomerName = x.Name,
                            CustomerAddress = x.Address,
                            CustomerPhone = x.Phone
                        }))
                        .ToArray();
                },
                mapperConfiguration =>
                {
                    mapperConfiguration.Transform.ToHashCode(source => source.Include(x => x.CustomerId), target => target.KeyHash);
                    mapperConfiguration.Transform.ToHashCode(source => source.All(), target => target.ChangeHash);
                })
            .LoadSqlite("Data Source=.\\target.db")
            .WithSlowlyChangingDimension();
    }
}
```

**Detailed Description**

1. Extract data from Sqlite in pages. The `@BatchSize` and `@Id` parameters are populated by the cursor pagination extractor.
2. Transform data from `SourceCustomer` to `TargetCustomer` with a hard coded property mapper. The `mapperConfiguration` defines a transform `ToHashCode` which automatically generates two separate hash codes for each row. 
    - A key hash is generated from the `CustomerId` property of `SourceCustomer` and assigned to the `KeyHash` property of `TargetCustomer`.
    - A change hash is generated from all properties of `SourceCustomer` and assigned to the `ChangeHash` property of `TargetCustomer`
3. Load data to Sqlite using the slowly changing dimension loader. This loader uses the previously generated hash values to determine whether to insert a new row or update an existing (changed) row.